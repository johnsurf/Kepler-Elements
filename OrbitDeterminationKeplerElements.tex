\section{Orbit Determination using Kepler Elements}

The object of this project is to develop an Orbit Determination (OD) method based on using Kepler Elements as State Vectors.\\

As regards possible motivations for using such state vectors, consider the standard Cartesian State Vectors and the methods used to represent the trajectory. Basically the standard approach considers the actual trajectory to be a kind of stochastic process where discrete observations are used to update the state vector. If such an approach was imagined to be taken to the limit of continuous updating, then it is very likely that the trajectory would become a stochastic process similar to Brownian motion except that the mean time dependence would be given by the assumed motion model. There is a fundamental mathematical problem which may be of concern for this approach and that is that even continuous stochastic processes such as Brownian motion are continuous but nowhere differentiable. Brownian motion can be considered as the ``integral'' of white noise and there is a method of mean-squared integration that yields the reverse -- that the mean-square ``derivative'' of Brownian motion is white noise. Therefore one wonders what significance the time-rate-of-change estimates that come out of updating a State Vector really are, or if there are fundamental limitations intrinsic to the method. Similar problems exist in the subject of polymer physics based on path-integral methods -- usually a "stiffness" term is inserted ad-hoc into the mathematics to insure some kind of limits on the fluctuations which are encountered in numerical work. Such stiffness terms in a polymer are similar to demanding some level of ``smoothness'' in addition to continuity. So too with orbit determination methods. Are there intrinsic limitations on the velocity determination which are due to the state vector update methods? The idea behind this study is to examine replacing Cartesian elements with a quasi-state state vector in terms of Kepler elements and adopt a parameter estimation approach which represents not only the trajectory but also all the time-derivatives of that trajectory in a smooth manner and see if the resulting fit has a better performance vis-a-vis the covariance structure in position as well as velocity space.\\

The data can be a combination of measurement variables including: position $\vec{r}$, velocity $\vec{v}$, range $\rho$, line-of-sight unit vectors (angles $\theta$ and $\phi$ -- or Az and El), range-rate $\dot\rho$, and angular rates $\dot\theta$ and $\dot\phi$ or any other quantity that can be expressed in terms of the osculating classical Kepler elements and time. We will start off by using only line-of-sight directions as the measurement data and assume that we are provided with line-of-sight unit vectors as measured by a moving observer -- which is not necessarily fixed on the surface of the earth. The unit vectors are assumed to be given in the Earth-Centered Inertial (ECI) frame and are determined by 2 angles which can be taken as Az and El or as $\theta$ and $\phi$ in spherical polar coordinates as measured in the ECI-referential frame attached to the sensor location (the coordinate system whose unit vectors are parallel to the ECI system, but carried along by the sensor). In this case the dimension of the measurement data is 2 corresponding to the angular information. But we will allow for more measurement flavors and multiple sensors. \\

One way to model the problem of OD is to find a method of predicting the time dependence of the angular variables represented as a column vector $\alpha(t_i) = [\theta(t), \phi(t)]$ for the line-of-sight in such a way to minimize the statistical measure $\chi^2$ defined by 
\begin{align*}\chi^2 = {1\over2}\sum_{i=1}^N\, [\alpha(t_i) - \talph_i]^T \hbox{Cov}_i^{-1}[\alpha(t_i) - \talph_i],\numberthis\label{eqnChiSq}\end{align*}where the sum is over the set of $N$ measurements $\talph_i = [\tthe_i, \tphi_i], t = 1,2\hdots, N$ and we use brackets $[ ~~ ]$ to denote column vectors denote transposition of a vector or a matrix by $[~~ ]^T$.This method leads to the usual batch Least Squares Estimate (LSE) approach and can be generalized to include other variables such as range, range-rate and the others mentioned in the list above by extending the vectors to dimension $n$ and the covariance structure to an $n$-by-$n$ real symmetric matrix.  The rank $n$ of Cov for each term on the Right Hand Side (RHS) of the above sum and the number of independent measurements in the data which are required in order to make an estimate of the full set of parameters. Typically the number of measured variables is less than the number of parameters being determined and therefore a minimum number of independent measurements are required before the problem reaches full rank and a estimate of the set of parameters possible. Of course, if an initial estimate of the state is available, then of course, updates to the parameter values can ensue via a sequential revision of the Least Squares method. \\

As an example, an extension of this problem to the list of variables mentioned at the beginning would require a measurement vector of the form 
$$\alpha = [\vec{r}, \vec{v}, \rho, \theta, \phi, \dot\rho, \dot\theta, \dot\phi]$$ at time $t$ and a corresponding prediction vector of the same quantities given in terms of the Kepler elements and the time $t$. This will be apparent to us as we proceed as the estimation process will involve solving a linear system and that linear system is constructed from a sum of terms each of which may have less rank than the set of parameters requires, but enough independent measurements are used so that the complete linear system has full rank. Otherwise the problem in batch mode is unsolvable -- we need to have enough data to perform the estimate from scratch. This will not preclude incorporating new data with less than full rank to an initial estimate. \\

It doesn't hurt to work out a general procedure for handling many flavors of measurements and then apply it to lower dimensional problems by simply turning off some of the terms -- a procedure that can do more, can also do less. We will apply this problem to the case of line-of-sight measurements, by ignoring the irrelevant terms in the vector of observations.\\

Other method of approaching the OD problem might use the Minimum Variance Estimate (MVE), or Maximum Likelihood (ML), or Restricted Maximum Likelihood (REML). We will work out a method based in minimizing $\chi^2$ which has ML as its counterpart, MVE and REML are analogous. In REML the object is to minimize the ``error contrasts'' which is similar to minimizing the variances occurring in the problem.\\

The next thing to notice is that the minimization of $\chi^2$ has several manifestations. One can attempt to minimize $\chi^2$ by computing its derivatives with respect to the parameter set 
$Z = [z_1, z_2, z_3, \hdots , zm]$ by computing the derivative vector and solving for 
\begin{align*}{\partial \chi^2\over \partial z_j} = 0,~~j = 1,2,\hdots m \numberthis \label{eqnFirstOrder}\end{align*}
This method usually is based on a first order Taylor expansion of the prediction vector $[\theta(t), \phi(t)]$ in terms of the parameter set $Z$ and, in the case of nonlinear functions, performing iterations via the Newton-Raphson technique. 
Application of the first order technique can be found in \cite{Avery} to vertex fitting methods in High Energy Physics which are the underlying theory for the SCOUT methods\cite{SQUAW}\\

This isn't the only way of setting up the minimization problem. For example one can look at the problem from the point of view of finding the critical points of the gradient of $\chi^2$ itself without worrying first about whether $\chi^2$ is a minimum. In other words, there is a second order method which seeks to solve for the critical points by expanding the gradient in its own linear Taylor expansion, viz., 
$${\partial \chi^2\over \partial z_j} \approx {\partial \chi^2\over \partial z_j} \biggl|_{\hat{Z}} + \sum_{k=1}^N  {\partial^2 \chi^2\over \partial z_j \partial z_k} \biggl|_{\hat{Z}}\,\delta z_k,\quad j =1,2,\hdots, m,$$
where $\delta z_k = z_k - \hat{z}_k$ and $\hat{Z}$ is the expansion point for the Taylor series.  In this method the idea is to apply Newton-Raphson iteration to the equation 
$$0 = {\partial \chi^2\over \partial z_j} \biggl|_{\hat{Z}} + \sum_{k=1}^N  {\partial^2 \chi^2\over \partial z_j \partial z_k} \biggl|_{\hat{Z}}\,\delta z_k,\quad j =1,2,\hdots, m,$$which can be turned into a linear system to solve for the increment $\delta z_k$ to be used as the expansion point $$\hat{Z} \rightarrow \hat{Z} + \delta z$$to be used in the next iteration. This updating is continued via Newton-Raphson iteration until (hopefully) the method converges and $\delta Z \rightarrow \bf{0}$ (the zero vector). The second order method takes more information into account, for example it uses the information on how the derivatives of $\chi^2$ themselves are changing as a function of the parameter set. Therefore this method also requires computing the Hessian matrix, $\bf H$, whose components are given by
$$H_{ij} = {\partial^2 \chi^2\over \partial z_j \partial z_k}, \quad i,j = 1,2,\hdots m$$

We will proceed to outline both the first order method as well as the second order method with respect to the prediction vector for the line-of-sight prediction vector $[\theta(t), \phi(t)]$. To apply the first order method we make the assumption that the linear approximation to the prediction vector near the minimum of $\chi^2$ is ``good enough'' meaning that is is ``reasonably linear'' where the minimum of $\chi^2$ is located. The may not be the case especially if the parameter space has hard boundaries and the minimum is located near those boundaries -- various problems can occur, e.g, the parameters may become complex numbers due to exceeding the physical range of possibilities.\\

